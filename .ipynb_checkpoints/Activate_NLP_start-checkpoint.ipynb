{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Python for Data Scientist: An Introduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module, we will use Natural Language ToolKit along with several other popular Python packages to build a data science pipeline to plot frequency histograms of words in html novels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you will need a Python installation (3.6.3 or later is recommended).\n",
    "```\n",
    "$ python --version\n",
    "3.6.3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone or download the repository https://github.com/LSU-Analytics/activate_2018.git\n",
    "```\n",
    "$ git clone https://github.com/LSU-Analytics/activate_2018.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to install the packages: \n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Or you can install the packages individually.\n",
    "```\n",
    "$ pip install beautifulsoup4\n",
    "$ pip install jupyter\n",
    "$ pip install matplotlib\n",
    "$ pip install nltk\n",
    "$ pip install pandas\n",
    "$ pip install requests\n",
    "$ pip install scipy\n",
    "$ pip install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data\n",
    "Where do we get data?  That's easy...data is everywhere.  We can import files (csv, xlsx, txt), pull from APIs (usually as JSON), or obtain raw HTML.  For this example, we will use the freely available online at Project Gutenberg.\n",
    "\n",
    "Here are several links to well known HTML books:\n",
    "- 'https://www.gutenberg.org/files/514/514-h/514-h.htm' # Little Women\n",
    "- 'https://www.gutenberg.org/files/42671/42671-h/42671-h.htm' # Pride & Prejudice\n",
    "- 'https://www.gutenberg.org/files/203/203-h/203-h.htm' # Uncle Tom's Cabin\n",
    "- 'https://www.gutenberg.org/files/205/205-h/205-h.htm' # Walden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to fetch the HTML file.  To do this, we will use a popular package known as ```requests```.  If you are familiar with http requests, we will be submitting a ```GET``` request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `requests`\n",
    "\n",
    "\n",
    "# Make the request and check object type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```type``` command outputs the datatype.  Here we are getting a ```Response`` object.\n",
    "\n",
    "The following commands extract and outputs the raw HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HTML from Response object and print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle the data\n",
    "\n",
    "**Tag soup** refers to unstructured (or malformed) HTML code.  The package ```BeautifulSoup``` allows you to easily interact with this code.\n",
    "\n",
    "Because we are in Lousiana, let's refer to our HTML soup as 'gumbo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup from bs4\n",
    "\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our ```gumbo``` object, we can extract some information such as title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get title as string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the hyperlinks within a page (< a > tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyperlinks from gumbo and check out first several\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For this project, we want the text from the ```gumbo``` object.  Luckily, there is a ```.get_text()``` method for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text out of the gumbo and print it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there!  While we have the text of the novel, it still contains some metadata.  Since the metadata is minimal and will not influence our findings, let's move forward witht he project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Words\n",
    "Next, we will use ```nltk``` tokenize text and remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex package\n",
    "\n",
    "\n",
    "# Define sentence\n",
    "\n",
    "\n",
    "# Define regex\n",
    "\n",
    "\n",
    "# Find all words in sentence that match the regex and print them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all words and print them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something similar with the ```text``` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all words in Moby Dick and print several\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that there is also a way to do this with ```nltk```, the Natural Language Toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RegexpTokenizer from nltk.tokenize\n",
    "\n",
    "# Create tokenizer\n",
    "\n",
    "\n",
    "# Create tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there!  At this point, words that start with a capital letter will be counted a separate instance.  To handle this issue, make all of the words lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new list\n",
    "\n",
    "\n",
    "# Loop through list tokens and make lower case\n",
    "\n",
    "\n",
    "# Print several items from list as sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words provide no real insights so let's remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk\n",
    "\n",
    "\n",
    "# Get English stopwords and print some of them\n",
    "\n",
    "\n",
    "# If you encounter an error, run the command below.\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new list\n",
    "\n",
    "\n",
    "# Add to words_ns all words that are in words but not in sw\n",
    "\n",
    "\n",
    "# Print several list items as sanity check\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering the question\n",
    "We started this project wanting to know the most frequently used words in a novel.  An easy manner to answer this question is to create a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import datavis libraries\n",
    "\n",
    "\n",
    "# Figures inline and set visualization style\n",
    "\n",
    "\n",
    "# Create freq dist and plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Create a reusable function\n",
    "\n",
    "There are hundreds of novels on Project Gutenbergso it makes sense to write a function that does utilizes our code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_freq(url, num = 25):\n",
    "    \"\"\"Takes a url & frequency and plots the word distribution\"\"\"\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_freq('https://www.gutenberg.org/files/521/521-h/521-h.htm', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "What have we learned?  You now have the foundation for 'scraping' HTML data from a website, extracting data, manipulating text, and plotting output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
